{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a267b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:26:23.230544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/benjamin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/benjamin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073c72f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    '/home/benjamin/Documents/OpenClassroomsDatasets/sentiment/sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "    encoding = \"ISO-8859-1\",\n",
    "    names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    ")\n",
    "data = data.drop(columns=[\"id\", \"date\", \"flag\", \"user\"])\n",
    "\n",
    "data.target = data.target.map(\n",
    "    {\n",
    "        0: 0.0, #Negative\n",
    "        2: 0.0, #Neutral\n",
    "        4: 1.0, #Positive\n",
    "    }\n",
    ")\n",
    "\n",
    "def sample_equal_classes(df, n_pos=100000, n_neg=100000):\n",
    "    df_pos = df[df[\"target\"] == 1.0].sample(n=n_pos)\n",
    "    df_neg = df[df[\"target\"] == 0.0].sample(n=n_neg)\n",
    "    \n",
    "    return pd.concat([df_pos, df_neg]).reset_index(drop=True)\n",
    "sampled_df = sample_equal_classes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c184f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>bet ur missus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>worri thing life dont pretend person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>cant wait til sister get hous dead wii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>hmm one seem get wear flatter way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>use iphon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                    text\n",
       "0     1.0                           bet ur missus\n",
       "1     1.0    worri thing life dont pretend person\n",
       "2     1.0  cant wait til sister get hous dead wii\n",
       "3     1.0       hmm one seem get wear flatter way\n",
       "4     1.0                               use iphon"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tweeter(sentence):\n",
    "    stemmer = PorterStemmer()\n",
    "    tk = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "    tok_sent = tk.tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [stemmer.stem(word.lower()) \n",
    "            for word in tok_sent \n",
    "            if word not in stop_words\n",
    "            and word.isalpha()==True]\n",
    "    sent=\"\"\n",
    "    for word in text:\n",
    "        sent+=word+\" \"\n",
    "    return sent[:-1]\n",
    "\n",
    "sampled_df[\"text\"] = sampled_df[\"text\"].apply(lambda x:tweeter(x))\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13c03981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.44081815 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "153248    0.0\n",
      "67802     1.0\n",
      "148889    0.0\n",
      "103093    0.0\n",
      "104681    0.0\n",
      "         ... \n",
      "119879    0.0\n",
      "103694    0.0\n",
      "131932    0.0\n",
      "146867    0.0\n",
      "121958    0.0\n",
      "Name: target, Length: 160000, dtype: float64\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:27:30.564723: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.6065 - accuracy: 0.6482 - val_loss: 0.5992 - val_accuracy: 0.6540\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.6005 - accuracy: 0.6551 - val_loss: 0.5988 - val_accuracy: 0.6550\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.5975 - accuracy: 0.6576 - val_loss: 0.5978 - val_accuracy: 0.6575\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.5956 - accuracy: 0.6592 - val_loss: 0.5976 - val_accuracy: 0.6589\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.5938 - accuracy: 0.6599 - val_loss: 0.5968 - val_accuracy: 0.6578\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.5920 - accuracy: 0.6623 - val_loss: 0.5993 - val_accuracy: 0.6579\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.5908 - accuracy: 0.6634 - val_loss: 0.6006 - val_accuracy: 0.6570\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.5897 - accuracy: 0.6642 - val_loss: 0.5981 - val_accuracy: 0.6576\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.5886 - accuracy: 0.6653 - val_loss: 0.6008 - val_accuracy: 0.6567\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.5875 - accuracy: 0.6661 - val_loss: 0.5995 - val_accuracy: 0.6579\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.01)\n",
    "X = vectorizer.fit_transform(sampled_df[\"text\"]).toarray()\n",
    "y = sampled_df[\"target\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "model.save('model/neural_network_model.h5')\n",
    "pickle_out = open(\"model/tfidf_vectorizer.pkl\",\"wb\")\n",
    "pickle.dump(vectorizer, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d989d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decision(predictions, threshold=0.5):\n",
    "    label=[]\n",
    "    for prediction in predictions:\n",
    "        label.append([1]) if prediction > threshold else label.append([0])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04ee2ccf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 2s 2ms/step\n",
      "Accuracy: 0.6579\n",
      "Confusion Matrix:\n",
      " [[10948  9059]\n",
      " [ 4625 15368]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.55      0.62     20007\n",
      "         1.0       0.63      0.77      0.69     19993\n",
      "\n",
      "    accuracy                           0.66     40000\n",
      "   macro avg       0.67      0.66      0.65     40000\n",
      "weighted avg       0.67      0.66      0.65     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "decisions = make_decision(y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, decisions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, decisions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, decisions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
